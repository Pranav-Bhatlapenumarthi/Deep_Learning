{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNytqyoZlEbL9b3t+rfPnaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranav-Bhatlapenumarthi/Deep_Learning/blob/main/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hja4N8qco6qX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1)  Defining the Model class and importing required dataset"
      ],
      "metadata": {
        "id": "3X0gFSvwDusV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  '''\n",
        "  This neural network as 2 hidden layers. It uses the ReLU activation function for the hidden layers.\n",
        "  '''\n",
        "  # Model class constructor\n",
        "  def __init__(self, input_features = 4, layer1 = 8, layer2 = 9, output_features = 3):\n",
        "    super(Model, self).__init__() # Instantiates nn.Module\n",
        "    self.fc1 = nn.Linear(input_features, layer1)\n",
        "    self.fc2 = nn.Linear(layer1, layer2)\n",
        "    self.out = nn.Linear(layer2, output_features)\n",
        "\n",
        "  # Function defining the forward pass\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.out(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "8XmFT510N0dL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1) We now create an instance of the model and pick a manual seed for randomization of the input."
      ],
      "metadata": {
        "id": "qBYOdNgEGW6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(20)\n",
        "model = Model()\n",
        "dataset = sns.load_dataset(\"iris\") # loading the IRIS dataset as a pandas dataframe\n",
        "type(dataset)\n",
        "dataset.tail()"
      ],
      "metadata": {
        "id": "D2mKFE5rDGEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2) Converting the 'species' values for strings to integer for computational convenience"
      ],
      "metadata": {
        "id": "Nj2M3hEM9pBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petal_type_to_num = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
        "dataset['species'] = dataset['species'].map(petal_type_to_num)\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "EGcVw8a05k50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3) Categorising training data and corresponding targets"
      ],
      "metadata": {
        "id": "M8cwkh_v-PGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset.drop('species', axis=1) # Training dataset\n",
        "y = dataset['species'] # Targets\n",
        "X.values; y.values # Converting to numpy arrays\n",
        "print(X, y)"
      ],
      "metadata": {
        "id": "mOSSVdFk6SqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4) Splitting the training data into training and testing sets"
      ],
      "metadata": {
        "id": "IB3LUpEw-bpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)"
      ],
      "metadata": {
        "id": "i_M6Ko4o7j61"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5) Converting training and test datasets to tensors"
      ],
      "metadata": {
        "id": "Fs6owVwX-3_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.FloatTensor(X_train.values)\n",
        "X_test = torch.FloatTensor(X_test.values)\n",
        "y_train = torch.LongTensor(y_train.values)\n",
        "y_test = torch.LongTensor(y_test.values)"
      ],
      "metadata": {
        "id": "P3YyUMc18ch-"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}